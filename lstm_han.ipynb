{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed,Dropout\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from random import shuffle\n",
    "from keras.optimizers import RMSprop,Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PREPROCESS(lines):    \n",
    "    data_n = len(lines) - 1\n",
    "    SEQ = np.zeros((data_n, 28, 4), dtype=int)\n",
    "    Score = np.zeros((data_n, 1), dtype=float)\n",
    "    shuffle(lines[1:])\n",
    "    for l in range(1, data_n+1):\n",
    "        data = lines[l].split(',')\n",
    "        seq = data[6]\n",
    "        Score[l-1] = float(data[4])\n",
    "        for i in range(28):\n",
    "            if seq[i] in \"Aa\":\n",
    "                SEQ[l-1, i, 0] = 1\n",
    "            elif seq[i] in \"Cc\":\n",
    "                SEQ[l-1, i, 1] = 1\n",
    "            elif seq[i] in \"Gg\":\n",
    "                SEQ[l-1, i, 2] = 1\n",
    "            elif seq[i] in \"Tt\":\n",
    "                SEQ[l-1, i, 3] = 1\n",
    "        #CA[l-1,0] = int(data[2])*100\n",
    "\t\n",
    "    return SEQ, Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34200\n"
     ]
    }
   ],
   "source": [
    "FILE = open(\"/home/csgrads/dbais001/CRISP-CAS9/yeast_bact/yeast_data/yeast_kmcs_cont_w.csv\", \"r\")\n",
    "data = FILE.readlines()\n",
    "FILE.close()\n",
    "print(len(data))\n",
    "SEQ, score = PREPROCESS(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20520, 28, 4)\n",
      "(20520, 1)\n",
      "(6840, 28, 4)\n",
      "(6840, 1)\n",
      "(6839, 28, 4)\n",
      "(6839, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x = SEQ[0:20520,:]\n",
    "train_y = score[0:20520]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "val_x = SEQ[20520:27360,:]\n",
    "val_y = score[20520:27360]\n",
    "print(val_x.shape)\n",
    "print(val_y.shape)\n",
    "test_x = SEQ[27360:,:]\n",
    "test_y = score[27360:]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 4)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 28, 32)            2688      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 28, 16)            2624      \n",
      "_________________________________________________________________\n",
      "attention_with_context_1 (At (None, 16)                288       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                1360      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 40)                1640      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 11,881\n",
      "Trainable params: 11,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SEQ = Input(shape=(28,4))\n",
    "blstm_1 = Bidirectional(LSTM(units=16, dropout=0.0,recurrent_dropout=0.0, return_sequences=True))(SEQ)\n",
    "blstm_2 = Bidirectional(LSTM(units=8,dropout=0.0, return_sequences=True))(blstm_1)\n",
    "#flatten = Flatten()(attn)\n",
    "attn = AttentionWithContext()(blstm_2)\n",
    "dropout_1 = Dropout(0.5)(attn)\n",
    "dense_1 = Dense(80, activation='relu', kernel_initializer='glorot_uniform')(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)\n",
    "dense_2 = Dense(units=40,  activation=\"relu\",kernel_initializer='glorot_uniform')(dropout_2)\n",
    "dropout_3 = Dropout(0.3)(dense_2)\n",
    "dense_3 = Dense(units=40,  activation=\"relu\",kernel_initializer='glorot_uniform')(dropout_3)\n",
    "out = Dense(units=1,  activation=\"linear\")(dense_3)\n",
    "model = Model(SEQ,out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20520 samples, validate on 6840 samples\n",
      "Epoch 1/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 6.2160Epoch 00000: val_loss improved from inf to 5.91735, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 12s - loss: 6.2066 - val_loss: 5.9173\n",
      "Epoch 2/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 5.7668Epoch 00001: val_loss improved from 5.91735 to 5.41828, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 5.7609 - val_loss: 5.4183\n",
      "Epoch 3/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 5.1520Epoch 00002: val_loss improved from 5.41828 to 4.78128, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 5.1525 - val_loss: 4.7813\n",
      "Epoch 4/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.8234Epoch 00003: val_loss improved from 4.78128 to 4.67353, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.8196 - val_loss: 4.6735\n",
      "Epoch 5/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.7070Epoch 00004: val_loss improved from 4.67353 to 4.45554, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.7070 - val_loss: 4.4555\n",
      "Epoch 6/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.4905Epoch 00005: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 4.4831 - val_loss: 4.5654\n",
      "Epoch 7/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.4448Epoch 00006: val_loss improved from 4.45554 to 4.33713, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.4478 - val_loss: 4.3371\n",
      "Epoch 8/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.3293Epoch 00007: val_loss improved from 4.33713 to 4.23576, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.3315 - val_loss: 4.2358\n",
      "Epoch 9/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.2701Epoch 00008: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 4.2707 - val_loss: 4.5640\n",
      "Epoch 10/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.2132Epoch 00009: val_loss improved from 4.23576 to 4.20201, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.2248 - val_loss: 4.2020\n",
      "Epoch 11/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.1866Epoch 00010: val_loss improved from 4.20201 to 4.12829, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.1914 - val_loss: 4.1283\n",
      "Epoch 12/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.0689Epoch 00011: val_loss improved from 4.12829 to 4.08450, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.0689 - val_loss: 4.0845\n",
      "Epoch 13/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.1131Epoch 00012: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 4.1057 - val_loss: 4.1349\n",
      "Epoch 14/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.0860Epoch 00013: val_loss improved from 4.08450 to 4.03842, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.0789 - val_loss: 4.0384\n",
      "Epoch 15/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.0637Epoch 00014: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 4.0709 - val_loss: 4.0441\n",
      "Epoch 16/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.0385Epoch 00015: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 4.0354 - val_loss: 4.0616\n",
      "Epoch 17/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 4.0202Epoch 00016: val_loss improved from 4.03842 to 3.98875, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 4.0177 - val_loss: 3.9888\n",
      "Epoch 18/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9975Epoch 00017: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9987 - val_loss: 4.1070\n",
      "Epoch 19/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9997Epoch 00018: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9994 - val_loss: 4.1111\n",
      "Epoch 20/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9868Epoch 00019: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9775 - val_loss: 4.0229\n",
      "Epoch 21/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9723Epoch 00020: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9862 - val_loss: 4.2476\n",
      "Epoch 22/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9367Epoch 00021: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9391 - val_loss: 4.0661\n",
      "Epoch 23/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9446Epoch 00022: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9482 - val_loss: 4.0586\n",
      "Epoch 24/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9352Epoch 00023: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9297 - val_loss: 4.1434\n",
      "Epoch 25/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.9024Epoch 00024: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.9017 - val_loss: 4.0464\n",
      "Epoch 26/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8973Epoch 00025: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8926 - val_loss: 4.2166\n",
      "Epoch 27/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8809Epoch 00026: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8810 - val_loss: 4.0651\n",
      "Epoch 28/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8852Epoch 00027: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8874 - val_loss: 3.9914\n",
      "Epoch 29/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8647Epoch 00028: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8584 - val_loss: 3.9954\n",
      "Epoch 30/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8328Epoch 00029: val_loss improved from 3.98875 to 3.93149, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.8282 - val_loss: 3.9315\n",
      "Epoch 31/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8312Epoch 00030: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8330 - val_loss: 3.9617\n",
      "Epoch 32/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8682Epoch 00031: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8612 - val_loss: 3.9373\n",
      "Epoch 33/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.8431Epoch 00032: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.8400 - val_loss: 3.9448\n",
      "Epoch 34/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7888Epoch 00033: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.7820 - val_loss: 3.9617\n",
      "Epoch 35/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7877Epoch 00034: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.7908 - val_loss: 4.0169\n",
      "Epoch 36/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7830Epoch 00035: val_loss improved from 3.93149 to 3.91922, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.7811 - val_loss: 3.9192\n",
      "Epoch 37/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7676Epoch 00036: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.7660 - val_loss: 3.9986\n",
      "Epoch 38/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7422Epoch 00037: val_loss improved from 3.91922 to 3.90572, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.7379 - val_loss: 3.9057\n",
      "Epoch 39/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7013Epoch 00038: val_loss improved from 3.90572 to 3.82466, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.7059 - val_loss: 3.8247\n",
      "Epoch 40/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7120Epoch 00039: val_loss improved from 3.82466 to 3.82233, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.7125 - val_loss: 3.8223\n",
      "Epoch 41/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.7145Epoch 00040: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.7066 - val_loss: 3.8319\n",
      "Epoch 42/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.6738Epoch 00041: val_loss improved from 3.82233 to 3.78137, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.6760 - val_loss: 3.7814\n",
      "Epoch 43/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.6400Epoch 00042: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.6390 - val_loss: 3.8612\n",
      "Epoch 44/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.6210Epoch 00043: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.6286 - val_loss: 3.8029\n",
      "Epoch 45/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.6188Epoch 00044: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.6177 - val_loss: 3.8644\n",
      "Epoch 46/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.6067Epoch 00045: val_loss improved from 3.78137 to 3.73652, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.6038 - val_loss: 3.7365\n",
      "Epoch 47/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5630Epoch 00046: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5629 - val_loss: 3.7909\n",
      "Epoch 48/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5667Epoch 00047: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5645 - val_loss: 3.8844\n",
      "Epoch 49/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5538Epoch 00048: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5581 - val_loss: 3.7705\n",
      "Epoch 50/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5380Epoch 00049: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5375 - val_loss: 3.7891\n",
      "Epoch 51/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5606Epoch 00050: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5580 - val_loss: 3.8133\n",
      "Epoch 52/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5470Epoch 00051: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5405 - val_loss: 3.7829\n",
      "Epoch 53/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5017Epoch 00052: val_loss improved from 3.73652 to 3.73184, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.5073 - val_loss: 3.7318\n",
      "Epoch 54/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.5122Epoch 00053: val_loss improved from 3.73184 to 3.71486, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.5115 - val_loss: 3.7149\n",
      "Epoch 55/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4960Epoch 00054: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.5025 - val_loss: 3.7931\n",
      "Epoch 56/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4422Epoch 00055: val_loss improved from 3.71486 to 3.69494, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.4372 - val_loss: 3.6949\n",
      "Epoch 57/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4511Epoch 00056: val_loss improved from 3.69494 to 3.66573, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.4505 - val_loss: 3.6657\n",
      "Epoch 58/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4871Epoch 00057: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.4768 - val_loss: 3.6669\n",
      "Epoch 59/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4383Epoch 00058: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.4397 - val_loss: 3.6917\n",
      "Epoch 60/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4204Epoch 00059: val_loss improved from 3.66573 to 3.60693, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.4185 - val_loss: 3.6069\n",
      "Epoch 61/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4496Epoch 00060: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.4447 - val_loss: 3.6358\n",
      "Epoch 62/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3986Epoch 00061: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3990 - val_loss: 3.6717\n",
      "Epoch 63/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3904Epoch 00062: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3912 - val_loss: 3.6472\n",
      "Epoch 64/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.4128Epoch 00063: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.4204 - val_loss: 3.7818\n",
      "Epoch 65/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3761Epoch 00064: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3710 - val_loss: 3.6293\n",
      "Epoch 66/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3945Epoch 00065: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3877 - val_loss: 3.6423\n",
      "Epoch 67/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3688Epoch 00066: val_loss improved from 3.60693 to 3.60305, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.3682 - val_loss: 3.6031\n",
      "Epoch 68/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3488Epoch 00067: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3487 - val_loss: 3.6784\n",
      "Epoch 69/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3416Epoch 00068: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3390 - val_loss: 3.6294\n",
      "Epoch 70/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3435Epoch 00069: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3483 - val_loss: 3.7941\n",
      "Epoch 71/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3317Epoch 00070: val_loss improved from 3.60305 to 3.59445, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.3323 - val_loss: 3.5945\n",
      "Epoch 72/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3063- ETA: 1s - loss:  - ETAEpoch 00071: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3063 - val_loss: 3.6451\n",
      "Epoch 73/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3239Epoch 00072: val_loss improved from 3.59445 to 3.59293, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.3195 - val_loss: 3.5929\n",
      "Epoch 74/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3233Epoch 00073: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.3204 - val_loss: 3.6003\n",
      "Epoch 75/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3119Epoch 00074: val_loss improved from 3.59293 to 3.56773, saving model to cas9.hdf5\n",
      "20520/20520 [==============================] - 11s - loss: 3.3159 - val_loss: 3.5677\n",
      "Epoch 76/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2860Epoch 00075: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2875 - val_loss: 3.5973\n",
      "Epoch 77/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.3003Epoch 00076: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2957 - val_loss: 3.6721\n",
      "Epoch 78/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2765Epoch 00077: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2711 - val_loss: 3.6119\n",
      "Epoch 79/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2780Epoch 00078: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2811 - val_loss: 3.6711\n",
      "Epoch 80/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2577Epoch 00079: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2591 - val_loss: 3.6825\n",
      "Epoch 81/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2846Epoch 00080: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2838 - val_loss: 3.6432\n",
      "Epoch 82/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2714Epoch 00081: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2724 - val_loss: 3.6717\n",
      "Epoch 83/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2605Epoch 00082: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2604 - val_loss: 3.6819\n",
      "Epoch 84/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2227- ETA: 0s - loss: 3.21Epoch 00083: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2226 - val_loss: 3.6009\n",
      "Epoch 85/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1976Epoch 00084: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1996 - val_loss: 3.6820\n",
      "Epoch 86/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2064Epoch 00085: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1986 - val_loss: 3.6599\n",
      "Epoch 87/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2398Epoch 00086: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.2377 - val_loss: 3.6229\n",
      "Epoch 88/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.2070Epoch 00087: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1992 - val_loss: 3.6619\n",
      "Epoch 89/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1907Epoch 00088: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1880 - val_loss: 3.6516\n",
      "Epoch 90/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1843Epoch 00089: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1809 - val_loss: 3.7986\n",
      "Epoch 91/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1974Epoch 00090: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1994 - val_loss: 3.6392\n",
      "Epoch 92/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1635Epoch 00091: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1623 - val_loss: 3.7175\n",
      "Epoch 93/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1864Epoch 00092: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1912 - val_loss: 3.6900\n",
      "Epoch 94/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1088Epoch 00093: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1119 - val_loss: 3.8464\n",
      "Epoch 95/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1459Epoch 00094: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1499 - val_loss: 3.7136\n",
      "Epoch 96/150\n",
      "20416/20520 [============================>.] - ETA: 0s - loss: 3.1496Epoch 00095: val_loss did not improve\n",
      "20520/20520 [==============================] - 11s - loss: 3.1464 - val_loss: 3.7165\n",
      "Epoch 00095: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bed904490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr = 0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "checkpointer = ModelCheckpoint(filepath=\"cas9.hdf5\",verbose=1, monitor='val_loss',save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "model.fit([train_x], train_y, batch_size=64, epochs=150, shuffle=True, validation_data=( [val_x], val_y), callbacks=[checkpointer,earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testset\n",
      "SpearmanrResult(correlation=0.6119674658660605, pvalue=0.0)\n",
      "SpearmanrResult(correlation=0.6371169383494569, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "print('testset')\n",
    "pred_y = model.predict([test_x])\n",
    "#print('mse ' + str(mse(test_y, pred_y)))\n",
    "print(st.spearmanr(test_y, pred_y))\n",
    "y_pred_tr = model.predict([train_x])\n",
    "print(st.spearmanr(train_y, y_pred_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6636359350323923, 0.0)\n",
      "(0.741786108220231, 0.0)\n"
     ]
    }
   ],
   "source": [
    "pred_y = pred_y.flatten()\n",
    "test_y = test_y.flatten()\n",
    "print(st.pearsonr(test_y, pred_y))\n",
    "print(st.pearsonr(train_y.flatten(), y_pred_tr.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "han",
   "language": "python",
   "name": "han"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
